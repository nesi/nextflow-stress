// Nextflow config file for Slurm cluster
// Save this as nextflow.config in your pipeline directory

// Process defaults
process {
    executor = 'slurm'
    
    // Default resources
    cpus = 1
    memory = '4 GB'
    time = '1h'
    
    // Queue selection based on job duration
    queue = { task.time <= 1.h ? 'short' : task.time <= 8.h ? 'medium' : 'long' }
    
    // Output handling
    scratch = true  // Use local scratch space
    clusterOptions = '--exclusive=user'  // Prevent other users from using the node
    
    // Process-specific configs
    withName: 'shortCpuBursts' {
        cpus = 1
        memory = '1 GB'
        time = '15m'
        queue = 'short'
    }
    
    withName: 'memoryIntensiveJobs' {
        cpus = 1
        memory = { 4.GB * task.attempt }
        time = '30m'
        queue = 'highmem'
    }
    
    withName: 'cpuIntensiveJobs' {
        cpus = { Math.min(16, 2 * task.attempt) }
        memory = '4 GB'
        time = '2h'
        queue = 'highcpu'
    }
    
    withName: 'ioIntensiveJobs' {
        cpus = 2
        memory = '4 GB'
        time = '1h'
        queue = 'io'
    }
    
    withName: 'mixedResourceJobs' {
        cpus = { 2 * task.attempt }
        memory = { 2.GB * task.attempt }
        time = '1h'
        queue = 'medium'
    }
    
    withName: 'aggregationJob' {
        cpus = 1
        memory = '2 GB'
        time = '30m'
        queue = 'short'
        errorStrategy = 'retry'
        maxRetries = 3
    }
}

// Executor settings
executor {
    name = 'slurm'
    queueSize = 100           // Maximum number of jobs submitted at once
    submitRateLimit = '10/1s'  // Maximum submission rate (10 jobs per second)
    jobName = { "stress_${task.process.tokenize(':')[-1]}_${task.index}" }
    pollInterval = '15 sec'   // How often to check job status
    queueStatInterval = '5min' // How often to refresh queue status
    exitReadTimeout = '10min'  // How long to wait for files to be accessed after job completion
}

// Nextflow run settings
params {
    // Default parameters (can be overridden by command line)
    num_samples = 100
    mem_intensive = true
    cpu_intensive = true
    io_intensive = true
    max_cpus = 16
    max_memory = '32 GB'
    max_time = '2h'
    failure_rate = 0.05
    output_dir = 'results'
    large_files = false
    file_size = '1GB'
    short_jobs = 50
    medium_jobs = 30
    long_jobs = 20
    dependency_chains = true
    max_retry = 3
}

// Resource usage reporting
report {
    enabled = true
    file = "${params.output_dir}/reports/execution_report.html"
}

// Trace file for detailed metrics
trace {
    enabled = true
    file = "${params.output_dir}/reports/execution_trace.txt"
    fields = 'task_id,hash,native_id,process,tag,name,status,exit,submit,duration,realtime,cpus,memory,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes'
}

// Timeline visualization
timeline {
    enabled = true
    file = "${params.output_dir}/reports/execution_timeline.html"
}

// Workflow visualization
dag {
    enabled = true
    file = "${params.output_dir}/reports/execution_dag.html"
}

// Advanced settings
singularity {
    enabled = false  // Enable if using Singularity containers
    autoMounts = true
}

// Profiles for different execution environments
profiles {
    // Main profile for full stress test
    full {
        params.num_samples = 200
        params.short_jobs = 100
        params.medium_jobs = 60
        params.long_jobs = 40
    }
    
    // Quick test profile
    test {
        params.num_samples = 10
        params.short_jobs = 5
        params.medium_jobs = 3
        params.long_jobs = 2
        params.failure_rate = 0.1
    }
    
    // Memory-intensive profile
    memtest {
        params.num_samples = 50
        params.mem_intensive = true
        params.cpu_intensive = false
        params.io_intensive = false
        params.short_jobs = 10
        params.medium_jobs = 30
        params.long_jobs = 10
    }
    
    // CPU-intensive profile
    cputest {
        params.num_samples = 50
        params.mem_intensive = false
        params.cpu_intensive = true
        params.io_intensive = false
        params.short_jobs = 10
        params.medium_jobs = 10
        params.long_jobs = 30
    }
    
    // I/O-intensive profile
    iotest {
        params.num_samples = 50
        params.mem_intensive = false
        params.cpu_intensive = false
        params.io_intensive = true
        params.large_files = true
        params.file_size = '5GB'
        params.short_jobs = 10
        params.medium_jobs = 30
        params.long_jobs = 10
    }
}

// Run with increased parallelism
env {
    NXF_OPTS = '-Xms1g -Xmx4g'
    // Add environment variables that might be needed
}
